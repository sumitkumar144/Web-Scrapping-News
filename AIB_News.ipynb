{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Business News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import nltk as nltk\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_AIB_html_summary(AIB_html):\n",
    "    try:\n",
    "        AIB_html_sumarry = AIB_html.findAll(\"h5\")\n",
    "    except:\n",
    "        AIB_html_sumarry = \"No summary available\"\n",
    "    return AIB_html_sumarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_AIB_html_time(AIB_html):\n",
    "    try:\n",
    "        AIB_html_time = AIB_html.findAll(\"time\")\n",
    "    except:\n",
    "        AIB_html_time = \"No tile available\"\n",
    "    return AIB_html_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NewsHeadlines(summary_content):\n",
    "    try:\n",
    "        news_headline = summary_content.a.text\n",
    "    except:\n",
    "        news_headline = \"No News Headlines\"\n",
    "    return news_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NewsLinks(summary_content):\n",
    "    try:\n",
    "        news_link =\"https://aibusiness.com/\" + summary_content.a['href']\n",
    "    except:\n",
    "        news_link = \"No News Link\"\n",
    "    return news_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PublishedDate(time):\n",
    "    try:\n",
    "        published_date = time.text\n",
    "        published_date = parse(published_date).strftime(\"%d %b %Y \")\n",
    "    except:\n",
    "        published_date = \"No dates published\"\n",
    "    return published_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NewsDescriptions(news_content):\n",
    "    news_body = \"\"\n",
    "    try:\n",
    "        for paragraph in news_content.findAll('p')[2:]:\n",
    "                paragraph = paragraph.get_text()\n",
    "                news_body = news_body + paragraph\n",
    "    except:\n",
    "        news_body = \"No News Description\"\n",
    "    return news_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    try:\n",
    "        text_nopunct = ''\n",
    "        #text_nopunct = re.sub('['+string.punctuation+']', ' ', text)\n",
    "        text_nopunct = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    except:\n",
    "        text_nopunct = \"Text punctuation removal not working\"\n",
    "    return text_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to lower case\n",
    "def lowercase_token(tokens): \n",
    "    return [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stopwords\n",
    "def removestopwords(tokens, stoplist): \n",
    "    return [word for word in tokens if word not in stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lematization of words\n",
    "def process_lemmatize_words(filtered_words):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lematization of words verbs\n",
    "def process_lemmatize_verbs(lemmatize_words):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in lemmatize_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_News_to_excel(AIB_NEWS_DataFrame,FileName):\n",
    "    try:\n",
    "        AIB_NEWS_DataFrame.to_excel(excel_writer= FileName+\".xlsx\",sheet_name=FileName)\n",
    "        print(\"Files Saved Successfully\")\n",
    "    except:\n",
    "        print(\"Error occured in saving the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AIB_News(FileName):\n",
    "    AIB_NEWS_DataFrame = pd.DataFrame()\n",
    "    AIB_urls = []\n",
    "    for page_no in range(3):\n",
    "        AIB_urls.append(\"https://aibusiness.com/archives.asp?section_id=782&piddl_archivepage={}\".format(page_no))\n",
    "    \n",
    "    AIB_urls = AIB_urls[1:]\n",
    "    \n",
    "    for AIB_url in AIB_urls:\n",
    "        try:\n",
    "            req = urllib2.Request(AIB_url, headers=hdr)\n",
    "            uClient = uReq(req) # requesting the webpage from the internet\n",
    "            news_page = uClient.read() # reading the webpage\n",
    "            uClient.close\n",
    "            AIB_html = bs(news_page, \"html.parser\")\n",
    "        except:\n",
    "            print(\"Webpage Not Responding. Try Including Header\")\n",
    "        AIB_html_sumarry = extract_AIB_html_summary(AIB_html)\n",
    "        AIB_html_time = extract_AIB_html_time(AIB_html)\n",
    "        News_Headlines = []\n",
    "        News_Links = []\n",
    "        Dates_Published = []\n",
    "        News_Descriptions = []\n",
    "        for summary_content in AIB_html_sumarry:\n",
    "            #Get List Of News HeadLines\n",
    "            news_headline = get_NewsHeadlines(summary_content)\n",
    "            News_Headlines.append(news_headline)\n",
    "            \n",
    "            #Get List Of News Links\n",
    "            news_link = get_NewsLinks(summary_content)\n",
    "            News_Links.append(news_link)\n",
    "            \n",
    "        for time in AIB_html_time:\n",
    "            #Get Published Date\n",
    "            published_date = get_PublishedDate(time)\n",
    "            Dates_Published.append(published_date)\n",
    "            \n",
    "        for news_link in News_Links:\n",
    "            try:\n",
    "                newsRes = requests.get(news_link)\n",
    "                newsRes.encoding = 'utf-8'\n",
    "                news_content = bs(newsRes.text, \"html.parser\")\n",
    "            except:\n",
    "                print(\"No content for news\")\n",
    "            #Get news Description \n",
    "            news_description = get_NewsDescriptions(news_content)\n",
    "            News_Descriptions.append(news_description)\n",
    "        mydict = {\"News Link\": News_Links, \"News Headlines\": News_Headlines, \"Date Published\" : Dates_Published[:-1], \"News Description\":News_Descriptions, \"Domain\": \"Miscellaneous\", \"Company\": \"Miscellaneous\"}    \n",
    "        AIB_NEWS_DataFrame = pd.concat([AIB_NEWS_DataFrame, pd.DataFrame(mydict, index = None)])    \n",
    "    try:\n",
    "        AIB_NEWS_DataFrame = AIB_NEWS_DataFrame.rename(columns = {'News Description':'News_Description'})\n",
    "        AIB_NEWS_DataFrame['News_Description_Cleaned'] = AIB_NEWS_DataFrame['News_Description'].apply(lambda News_Description: remove_punctuation(News_Description))\n",
    "        ## Tokenize sentences\n",
    "        tokens = [word_tokenize(sen) for sen in AIB_NEWS_DataFrame.News_Description_Cleaned]\n",
    "        lower_tokens = [lowercase_token(token) for token in tokens]\n",
    "        stoplist = stopwords.words('english')\n",
    "        filtered_words = [removestopwords(sen, stoplist) for sen in lower_tokens]\n",
    "        lemmatize_words = [process_lemmatize_words(sen) for sen in filtered_words]\n",
    "        lemmatize_verbs = [process_lemmatize_verbs(sen) for sen in lemmatize_words]\n",
    "        AIB_NEWS_DataFrame['News_Description_Cleaned'] = [' '.join(sen) for sen in lemmatize_verbs]\n",
    "        AIB_NEWS_DataFrame['tokens'] = lemmatize_verbs\n",
    "    except:\n",
    "        print(\"There is some problem with text pre-processing\")\n",
    "        \n",
    "    save_News_to_excel(AIB_NEWS_DataFrame,FileName) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "AIB_News('AIB_NEWS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
